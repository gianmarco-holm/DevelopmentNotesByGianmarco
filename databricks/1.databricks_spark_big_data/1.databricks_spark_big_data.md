# Databricks y Apache Spark para Big Data

>1. Introducción a Spark

## 1.1. Fundamentos de SPARK

Apache Spark es un motor de procesamiento de datos rápido y de código abierto, diseñado para analizar grandes volúmenes de datos en sistemas distribuidos. Permite procesar datos de manera eficiente en la memoria RAM, lo que lo hace más rápido que Hadoop MapReduce, que depende del disco. Spark es popular en el análisis de big data, machine learning y procesamiento de datos en tiempo real, y soporta varios lenguajes de programación, incluyendo Python, Scala, Java y SQL.

## 1.2. Como se ejecuta Apache Spark

Para poder entender como se ejecuta, primero necesitamos saber algunos conceptos básicos:

* **Cluster:** Es un conjunto de varias computadoras (también llamadas nodos) que trabajan juntas para procesar datos y realizar tareas en paralelo.
* **Nodo o Worker:** Es una computadora dentro del clúster que ejecuta tareas asignadas por el driver

### Proceso de Ejecución

* Paso 1: Envío de la Aplicación: El usuario envía la aplicación Spark (escrita en Scala, Python, Java o R) al driver, que gestiona el trabajo.
* Paso 2: División del Trabajo: El driver divide el trabajo en varias tareas y planifica cómo ejecutarlas en paralelo.
* Paso 3: Asignación de Recursos: El driver solicita recursos al cluster manager, quien asigna nodos de trabajo (workers) para ejecutar las tareas.
* Paso 4: Ejecución de Tareas en los Workers: Los workers ejecutan las tareas en los executors, y los datos intermedios se almacenan en la memoria (o en el disco, si se requiere).
* Paso 5: Recopilación de Resultados: Los resultados parciales generados en los workers se envían al driver, quien los consolida y produce el resultado final.

## 1.3. Ecosistema de Apache Spark

* Spark Core: El núcleo de Spark, que gestiona la ejecución de * tareas distribuidas y el procesamiento de datos en paralelo.
* Spark SQL: Realiza consultas SQL sobre datos estructurados y trabaja con DataFrames.
* Spark Streaming: Procesa flujos de datos en tiempo real, útil para análisis continuo.
* MLlib: Biblioteca de machine learning con algoritmos para big data.
* GraphX: Herramienta para análisis de grafos y redes.
* Structured Streaming: Versión mejorada para procesamiento continuo de datos estructurados.
* SparkR y PySpark: APIs para usuarios de R y Python, respectivamente.
* Scala y Java APIs: Acceso a todas las funcionalidades de Spark en sus lenguajes base.

>2. Spark Dataframe y Apache Spark SQL

## 2.1. Fundamentos y ventajas de los dataframes

| Característica        | RDD                             | DataSet                        | DataFrame                     |
|-----------------------|---------------------------------|--------------------------------|-------------------------------|
| **Lenguaje**          | Scala, Java, Python             | Scala, Java, Python            | Scala, Java, Python, R        |
| **Tipo de Datos**     | No estructurados (objetos)      | Estructurados con tipos        | Estructurados (tablas)        |
| **Facilidad de Uso**  | Baja (requiere código más complejo) | Moderada (requiere uso de tipos) | Alta (similar a SQL)          |
| **Rendimiento**       | Bajo (sin optimización)        | Mejor que RDD                   | Mejor (optimizado con Catalyst)|
| **Optimización**      | No tiene optimización automática | Usa Catalyst para optimizar    | Usa Catalyst para optimizar   |
| **Comodidad**         | Baja (requiere más esfuerzo)    | Moderada (requiere trabajar con tipos) | Alta (fácil de usar con SQL)  |
| **Uso Común**         | Manipulación de datos complejos | Trabajo con datos estructurados y tipo seguro | Análisis de datos estructurados |

## 2.2. Características del Dataframe

1. Distribuidos:

Los DataFrames están distribuidos a través de un clúster de máquinas. Esto significa que los datos se dividen en varias particiones y se procesan de forma paralela en diferentes nodos, permitiendo escalar el procesamiento de grandes volúmenes de datos.

2. Evaluación Perezosa (Lazy Evaluation):

Spark aplica una evaluación perezosa, lo que significa que las transformaciones en los DataFrames (como filter, map, etc.) no se ejecutan inmediatamente. En su lugar, Spark construye un plan de ejecución y lo ejecuta solo cuando se requiere el resultado (por ejemplo, cuando se llama a show() o collect()).

3. Inmutabilidad:

Los DataFrames son inmutables, lo que significa que no puedes cambiar los datos de un DataFrame una vez creado. Si realizas una transformación, Spark crea un nuevo DataFrame con los resultados, dejando el original intacto.

4. Tolerancia a Fallos:

Los DataFrames en Spark son tolerantes a fallos debido a la arquitectura de Spark basada en RDDs. Si un nodo falla durante el procesamiento, Spark puede recomputar las particiones perdidas a partir de los datos originales utilizando información de 
metadatos almacenados.

## 2.3. Creación de Dataframe

```python
import findspark
from pyspark.sql import SparkSession

spark = SparkSession.builder.getOrCreate()

df = spark.createDataFrame(datos, columnas)
# Donde datos es una lista de tuplas, y columnas es una lista
```

## 2.4. Operaciones básicas en DataFrames

[enlace](https://drive.google.com/file/d/1n2mJXOOvOu1r_3ar4CZPBIIstrROiqVJ/view?usp=sharing)