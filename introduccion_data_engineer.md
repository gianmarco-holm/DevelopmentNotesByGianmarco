#  Fundamentos de Ingenier铆a de Datos

## Tabla de Contenido

[TOC]

## Data Engineer: 驴Qu茅 es?

La Ingenier铆a de Datos es una disciplina que se enfoca en el desarrollo y gesti贸n de arquitecturas de datos, pipelines de procesamiento y sistemas para analizar y visualizar informaci贸n. Un Data Engineer es un profesional especializado en dise帽ar, construir y mantener estas infraestructuras.

![data engineer](./images/dataEngineer.png)

## 驴C贸mo Convertirse en Data Engineer?

1. **Educaci贸n y Habilidades:**
    - Adquirir conocimientos en bases de datos, programaci贸n y procesamiento de datos.
    - Dominar lenguajes como Python, SQL, y herramientas como Apache Spark.

2. **Experiencia Pr谩ctica:**
    - Participar en proyectos de ingenier铆a de datos.
    - Crear y mantener pipelines de datos.

3. **Formaci贸n Continua:**
    - Mantenerse al tanto de las 煤ltimas tecnolog铆as y tendencias en el campo.

### 驴D贸nde ejercer como Data Engineer?

*StartUp
*Empresas de software
*Corporaciones

##  Integraci贸n de DevOps, Lean y Agile en DataOps

### DataOps: Resumen

DataOps es una metodolog铆a que busca mejorar la colaboraci贸n y eficiencia en el ciclo de vida de los datos. Combina pr谩cticas de desarrollo de software con operaciones de datos para lograr una entrega m谩s r谩pida y confiable de informaci贸n de calidad.

![dataops](./images/dataOps.PNG){width=500px}

## Integraci贸n de Metodolog铆as

###  Agile en DataOps

![Agile](./images/Agile.PNG){width=500px}

* **Scrum:**

Roles definidos (Scrum Master, Product Owner, Equipo de Desarrollo).
Iterativo e incremental en sprints.
Eventos predeterminados (Sprint Planning, Daily Scrum, Sprint Review, Sprint Retrospective).
Uso de Backlog de Producto y Sprint Backlog.
Estimaciones de tiempo (puntos de historia).
![Scrum](./images/scrum.jpeg){width=500px}

* **Kanban:**

Roles flexibles, sin roles predeterminados.
Flujo continuo de trabajo, sin iteraciones fijas.
No tiene eventos predeterminados.
Utiliza un tablero Kanban para visualizar el flujo de trabajo.
No tiene Sprint Backlog ni Backlog de Producto.
Evita estimaciones de tiempo, se centra en limitar el trabajo en curso (WIP).
![Kanban](./images/kanban.jpeg){width=500px}

* **Kanban:**
    -Trello
    -Jira
    -Asana

###  Lean en DataOps

- **Eliminaci贸n de Desperdicios:**
  - Identificar y eliminar actividades que no agregan valor.
  - Mejora continua en la entrega de datos.
![data engineer](./images/lean.PNG){width=500px}


###  DevOps en DataOps

- **Colaboraci贸n:**
  - Superar las barreras entre desarrollo y operaciones.
  - Automatizaci贸n en todas las etapas del ciclo de vida de los datos.
![data engineer](./images/devops.PNG){width=500px}

### Por Qu茅 se Integra

1. **Colaboraci贸n Efectiva:**
    - Rompe silos entre equipos para una colaboraci贸n m谩s efectiva.

2. **Automatizaci贸n Integral:**
    - La automatizaci贸n abarca desde la recolecci贸n hasta el monitoreo de datos.

3. **Entrega Continua:**
    - Facilita entregas continuas y r谩pidas de datos.

4. **Mejora Continua:**
    - Fomenta la mejora continua en todos los aspectos del ciclo de vida de los datos.

## Lengujes de Programaci贸n para Data Engineering

* Python:
>Nos ayuda mucho en Data Science, posee muchas librer铆as de c贸digo cient铆fico.
Puede ser m谩s lento que otras opciones.
Es muy sencillo.

*  R:
>Es un antecesor de Python.
Es muy 煤til para trabajar con estad铆sticas y modelos.
Importante para analistas.

* Scala:
>Usa java de base.
Con la implementaci贸n y optimizaci贸n de PySpark baj贸 su necesidad.
Interesante para programaci贸n funcional.

* Java:
>Scala corre sobre java.
Su escalabilidad es envidiable.
Puede ser un c贸modo siguiente paso.

*  JavaScript:
>Navaja suiza para web developers.
Super flexible y 煤til para muchos 谩mbitos.
Imponente por la cantidad de herramientas que tiene.
Visualizaciones de datos m谩s bellas posibles.

* C++ y derivados:
>Columna vertebral para muchos proyectos.
Curva de aprendizaje potente.
Muchas herramientas usan C en el fondo.
Implementaciones modernas ayudan a que no sea tan dif铆cil de implementar.

## Donde y como escribir codigo de ingenier铆a de datos

*  Jupyter notebooks : Jupyter Lab
*  IDE : Data Spell
*  Editor de codigo : VsCode
*  Complementos : Tener de amiga a la terminal, usar tambien git, github o gitlab.

## Automatizaci贸n y Scripting

- Porque debo automatizar las tareas?
    * Para trabajar de manera inteligente
    * Optimizar el proceso
    * Utilizar recursos externos
- Porque debo usar python?
    * Porque es un lenguaje sencillo
    * Librerias variadas
    * Comunidad activa
    * Multiplataforma
> **Cuando debemos automatizar una tarea?**
Cuando se aplique la regla Beetlejuice, que quiere decir que despues de repetir 3 veces la tarea, tendras mas claro lo que vas a automatizar y porque

## Fuentes de Datos

- SQL
    * Lenguaje de consulta pero tambien el nombre con que se identifican las bases de datos
    * Excelentes para las transacciones por los principios ACID (Atomicity, Consistency, Isolation, Durability)
- NoSQL
    * Por su cercania a los lenguajes de programacion son utiles para guardar objetos flexibles
    * La mas famosa es mongoDB, otras importantes como Redis, ElasticSearch, HBase
- API
    * Consume informacion de otras plataformas
    * Permite utilizar capacidades mandando un input, recibiendo un output
    * Pueden ser creadas por uno, externas y de paga
- Web scraping
    * Es traernos informacion disponible que esta en internet
    * parsehub, Scrapy

## Procesamiento de Datos

> El procesamiento de datos se refiere al conjunto de operaciones y transformaciones realizadas en datos para obtener informaci贸n 煤til. Incluye la recopilaci贸n, limpieza, an谩lisis y presentaci贸n de datos de manera que se puedan tomar decisiones informadas.
Para ello tenemos:
- Apache Spark:
Motor de procesamiento de datos en memoria y distribuido.

- Pipeline:
Secuencia l贸gica de etapas de procesamiento de datos, que se encarga de extraer, luego transforma y mandar la data trasnformada a otro lado.

- C贸mputo Paralelo (en Spark):
Realizaci贸n simult谩nea de tareas en m煤ltiples nodos para mayor eficiencia, es usada por apache spark.

### Pipelines

- Para automatizar pipelines nos surgen tres preguntas: 驴Qu茅 se va a correr? 驴En qu茅 secuencia? 驴Cu谩ndo y cada cuanto se va a correr?

Airflow nos ayuda a automatizar estas tareas, contest谩ndonos las 3 preguntas. El que apoy谩ndonos a conectar tareas de c贸digo que vamos a correr, conect谩ndolas en secuencia y tambi茅n con un manejador de tiempos para que se corra con una definici贸n de cada cu谩nto se van a correr todo esto por medio de una interfaz web muy sencilla.

En el formato DAG es como le hablamos a Airflow para que ejecute las tareas en un grafo ac铆clico dirigido podemos indicarle cuales tareas ir谩n al principio y cuales ir谩n al final, poniendo los pasos con las dependencias adecuadas.
![dag](./images/dag.jpeg){width=500px}

## Apache Airflow

- Orquestar y programar workflows de datos con Apache Airflow.

## Contenedores y Empaquetamiento: Docker y Kubernetes

- Empaquetar y distribuir aplicaciones y entornos con contenedores.

## Manejo de Ambientes

- Gestionar entornos de desarrollo, prueba y producci贸n de manera efectiva.

## Testing de Software y Datos

- Implementar pruebas para garantizar la calidad de software y datos.
    * Pruebas unitarias (de funciones)
    * Pruebas de integraci贸n ( para saber si integra bien con otras librerias, apis o aplicaciones)
    * Pruebas E2E para saber si el resultado es lo que esperamos
    * Y otras pruebas

## CI/CD

> Debemos de hacer integraciones continuas o entregas continuas y esta muy asociado a DevOps, es bueno conocerlo, pero habr谩 expertos en esto. DevOps est谩 enfocado en el ciclo de desarrollo de software y el proceso de despliegue de C贸digo a producci贸n. Mientras que DataOps est谩 enfocado en el ciclo de vida de los datos, se usa para garantizar la calidad de los datos y mejorar la velocidad en que estos son entregados.

## Servidores y Computaci贸n en la Nube para Data

- Alternativas del mercado y sus similitudes
    * AWS
    * Azure
    * Google Cloud

- Cosas a tener en cuenta
    * Manteniendo recursos
    * Manteniendo usuarios
    * Manteniendo costos
    * Distribuyendo carga
    * Consideraciones de seguridad

## Medici贸n de indicadores y seguimiento a proyectos
Es importante tener medidores para detectar problemas antes que crezcan.
- Puntos de riesgo:
    * A la entrada del servicio
    * A la entrada del modelo
    * Durante el proceso
    * Datos de consulta externa
    * A la salida del modelo
    * A la salida del servicio
    * Lo que s ele msotro al cliente

- Para tener visibiidad:
    * Tener dashboards
    * Tener alertas y notificaciones


## Buscando Oportunidades

- LinkedIn
- Meetup
- Torre
- GetOnBoard
- Aprender Ingles es importante

## Ganando Seniority

- El primer d铆a aprende
- El primer mes contribuye
- El primer a帽o mentorea
- Luego no tengas miedo de ir por m谩s

## Evoluci贸n en el rol

![Camino01](./images/Caminos%20para%20el%20Data%20Engineer.PNG){width=500px}
![Camino02](./images/camino2.jpeg){width=500px}

**Y hay un tercer camino:**
*  Cambiar a Ciencia de datos
*  Arquitectura
*  DevOps y Cloud

## Trabajando en equipo como Data Engineer
>Los data engineer trabajan en equipo, hay cuatro roles muy importantes Data Analyst, nos ayuda a presentar los datos, los ingenieros de machine learning nos ayudan a publicar modelos, los data Scientist nos ayudan en todo el proceso, pero no saben mucho publicar c贸digo en producci贸n con otros lenguajes de programaci贸n. Nuestro rol, es apoyar a los tres perfiles con procesos de datos adecuado, a poca latencia y muy escalables y estables, somos un soporte. Colaboramos con personas de producto y con el equipo de desarrollo de apps, para poder conectarnos a las fuentes de datos.

![posici贸n](./images/posici贸n.jpeg){width=500px}

>Pasos y roles en el flujo de trabajo del data sciencie

![flujoDataEngineeer](./images/flujo.jpeg){width=500px}
---

# DevOps y DataOps: Buscan mejorar la eficiencia y la calidad del trabajo

**DevOps** y **DataOps** son dos metodolog铆as diferentes que buscan mejorar la eficiencia y la calidad del trabajo en la gesti贸n de software y datos, respectivamente. Aunque tienen objetivos diferentes, ambos se centran en la automatizaci贸n, la colaboraci贸n y la mejora continua.

A continuaci贸n, se detallan las diferencias, similitudes y l铆mites de ambas metodolog铆as:

## Diferencias

- **DevOps** se enfoca en la automatizaci贸n del ciclo de vida del software, desde el desarrollo hasta la implementaci贸n y operaci贸n, mientras que **DataOps** se centra en la automatizaci贸n del ciclo de vida de los datos, desde la adquisici贸n hasta el an谩lisis y la entrega.

- **DevOps** busca mejorar la integraci贸n y la colaboraci贸n entre los equipos de desarrollo y operaciones, mientras que **DataOps** se enfoca en la colaboraci贸n entre los equipos de datos y de negocio.

- **DevOps** busca aumentar la velocidad y la calidad de la entrega de software, mientras que **DataOps** busca mejorar la calidad y la agilidad en la gesti贸n de datos.

## Similitudes

- Ambas metodolog铆as se basan en la automatizaci贸n y la mejora continua para mejorar la eficiencia y la calidad del trabajo.

- Ambas fomentan la colaboraci贸n y la integraci贸n entre los equipos de trabajo para lograr sus objetivos.

- Ambas buscan una entrega m谩s r谩pida y eficiente de sus respectivos productos, ya sea software o datos.

## L铆mites

- **DevOps** no se centra en la gesti贸n de datos y su calidad, mientras que **DataOps** no se enfoca en el ciclo de vida completo del software.

- Cada metodolog铆a tiene un conjunto diferente de herramientas y pr谩cticas que no son necesariamente aplicables a la otra.

- Ambas metodolog铆as pueden ser dif铆ciles de implementar en organizaciones con culturas y estructuras resistentes al cambio.

## 5 Herramientas para DevOps y DataOps:

### Herramientas de DevOps

1. **Jenkins:** Automatizaci贸n de integraci贸n continua y entrega continua.
2. **Docker:** Plataforma de contenedores para creaci贸n, distribuci贸n y ejecuci贸n de aplicaciones.
3. **Ansible:** Automatizaci贸n de TI para configuraci贸n, gesti贸n y orquestaci贸n de sistemas.
4. **GitLab:** Gesti贸n de repositorios de c贸digo con colaboraci贸n y seguimiento del ciclo de vida del software.
5. **Kubernetes:** Plataforma de orquestaci贸n de contenedores para gesti贸n y escalabilidad de aplicaciones.

### Herramientas de DataOps

1. **Airflow:** Plataforma de orquestaci贸n de flujo de trabajo para automatizaci贸n de procesos de datos.
2. **Databricks:** Plataforma de an谩lisis de datos que facilita la integraci贸n y colaboraci贸n entre equipos.
3. **Apache Kafka:** Plataforma de streaming de datos para procesamiento y entrega en tiempo real.
4. **Talend:** Herramienta de integraci贸n de datos para extracci贸n, transformaci贸n y carga de datos.
5. **Snowflake:** Plataforma de almacenamiento de datos en la nube para an谩lisis y gesti贸n segura y escalable de datos.

## Conclusi贸n

La Ingenier铆a de Datos es un campo din谩mico y esencial en el mundo de la ciencia de datos. El Data Engineer desempe帽a un papel crucial en la creaci贸n y mantenimiento de infraestructuras para el procesamiento y an谩lisis de datos. La combinaci贸n de habilidades t茅cnicas, experiencia pr谩ctica y la adopci贸n de metodolog铆as 谩giles son fundamentales para tener 茅xito en esta profesi贸n en constante evoluci贸n.
