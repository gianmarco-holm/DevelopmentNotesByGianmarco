# ETIQUETADO MORFOSINTACTICO

## 1. 📌 Morfosintaxis
La ``morfosintaxis`` es la rama de la lingüística que estudia la relación entre la ``morfología`` (estructura interna de las palabras) y la ``sintaxis`` (cómo se organizan las palabras en oraciones). Esto nos ayuda a determinar las partes de la oración llamadas ``part of speech (POS)``, y estas son las que se clasifican en ``categorías morfosintácticas o gramaticales.``

## 2. 📌 ¿Qué es el Etiquetado Morfosintáctico?  
El etiquetado morfosintáctico es el proceso de asignar una categoría gramatical a cada palabra en un texto. Se usa en PLN para tareas como análisis sintáctico, traducción automática y minería de texto.  

### 2.1. 🔍 Principales Categorías Morfosintácticas  
1. **Sustantivos (NOUN)** → Persona, lugar, cosa.  
2. **Adjetivos (ADJ)** → Describen características.  
3. **Verbos (VERB)** → Acción o estado.  
4. **Adverbios (ADV)** → Modifican verbos o adjetivos.  
5. **Pronombres (PRON)** → Sustituyen sustantivos.  
6. **Preposiciones (ADP)** → Relacionan palabras.  
7. **Conjunciones (CONJ)** → Unen frases u oraciones.  
8. **Determinantes (DET)** → Artículos y posesivos.  
9. **Interjecciones (INTJ)** → Expresiones emocionales.  

### 2.2. 🛠️ Herramientas para Etiquetado  
- **NLTK (Python)** → Modelos para etiquetado en español.  
- **spaCy (Python)** → Modelos neuronales avanzados.  
- **Stanford NLP (Java/Python)** → Soporte para español.  

> Hoy en día, la mayoría de los algoritmos de procesamiento del lenguaje natural que procesan palabras en inglés utilizan el **Penn Treebank**, que es un conjunto de 45 etiquetas.

---

## 3. 🏷️ Etiquetado Morfosintáctico con Modelos Ocultos de Markov (HMM)

Un ``Modelo Oculto de Markov (HMM)`` es un modelo probabilístico que representa una secuencia de estados ocultos a partir de observaciones visibles. En el contexto del ``Procesamiento del Lenguaje Natural (PLN)``, los ``estados ocultos`` son las categorías gramaticales y las ``observaciones`` son las palabras de una oración.  

### 3.1. 🔄 Funcionamiento del HMM para Etiquetado  

1. **Definir los estados** → Son las categorías gramaticales posibles (sustantivo, verbo, adjetivo...).  
2. **Probabilidades de transición** → Probabilidad de que una categoría gramatical suceda a otra.  
3. **Probabilidades de emisión** → Probabilidad de que una palabra pertenezca a una categoría.  
4. **Algoritmo de Viterbi** → Encuentra la secuencia más probable de etiquetas para una oración.  

### 📊 Ejemplo de Etiquetado con HMM

#### Frase: *El gato duerme tranquilo*  
| Palabra   | Posibles Etiquetas | Etiqueta Asignada |
| --------- | ------------------ | ----------------- |
| El        | DET, PRON          | DET               |
| gato      | SUST, VERB         | SUST              |
| duerme    | VERB, SUST         | VERB              |
| tranquilo | ADJ, ADV           | ADJ               |

🔹 **Explicación**: Se usan las probabilidades de transición y emisión para determinar la secuencia más probable de etiquetas.  

### 3.2. 🏗️ Construcción de un Etiquetador Morfosintáctico con un HMM Bigrama

Un ``Modelo Oculto de Markov (HMM) Bigrama`` es un modelo probabilístico en el que la categoría gramatical de una palabra depende únicamente de la categoría de la palabra anterior. Esto simplifica el modelo y lo hace eficiente para tareas de ``etiquetado morfosintáctico``.

1. El HMM tiene tantos estados como etiquetas, además de un estado inicial y final.
```
<s> en/IN un/DT vaso/NN de/IN vino/NN </s>
<s> un/DT mosquito/NN se/PRP cayó/VBZ </s>
<s> y/CC vino/VBZ otro/JJ mosquito/NN </s>
```
> En este caso tiene 7 estados en el corpus y 9 en total contando la del inicio y final.

2. Luego se calcula las probabilidad de transicion de todos los estados, por ejemplo de PRP seguido de NN

$$
P(T_i | T_{i-1}) = \frac{C(NN, PRP)}{C(NN)} = \frac{1}{4} = 0.25
$$

3. Este resultado se reemplaza en una matriz.

**Conclusión**: El etiquetado morfosintáctico con HMM es una técnica efectiva para analizar la estructura de los textos y es fundamental en el procesamiento del lenguaje natural.  

---

## 4. 🔢 Creación de la Matriz de Probabilidades de la Ruta de Viterbi

### 4.1. 📌 ¿Qué es el Algoritmo de Viterbi?
El ``algoritmo de Viterbi`` es un método eficiente para encontrar la secuencia más probable de estados ocultos en un ``Modelo Oculto de Markov (HMM)``. En el ``etiquetado morfosintáctico``, permite determinar la mejor secuencia de categorías gramaticales para una oración.

### 4.2. 🏗️ Construcción de la Matriz de Viterbi
El algoritmo utiliza una ``matriz de probabilidades`` donde las filas representan las ``categorías gramaticales`` y las columnas representan las ``palabras de la oración``.

**La matriz se calcula con:**
1. **Probabilidades de emisión** → Probabilidad de que una palabra tenga una etiqueta específica.
2. **Probabilidades de transición** → Probabilidad de que una etiqueta siga a otra.
3. **Paso de retroceso (backtracking)** → Encuentra la secuencia óptima de etiquetas.

### 4.3. 📊 Ejemplo Paso a Paso
Supongamos la oración:  
**"El gato duerme"**  
Categorías posibles:
- **DET** → Determinante
- **SUST** → Sustantivo
- **VERB** → Verbo

> La matrices se definen en base a un corpus inicial

#### 🔹 1️⃣ Definir Matrices de Probabilidad  

| Categoría | Probabilidad Inicial |
| --------- | -------------------- |
| DET       | 0.6                  |
| SUST      | 0.3                  |
| VERB      | 0.1                  |

**Probabilidad de Transición $P[Etiq_i | Etiq_{i-1}]$**

| De/Para | DET | SUST | VERB |
| ------- | --- | ---- | ---- |
| DET     | 0.1 | 0.8  | 0.1  |
| SUST    | 0.3 | 0.4  | 0.3  |
| VERB    | 0.5 | 0.2  | 0.3  |

**Probabilidad de Emisión $P[Palabra | Etiqueta]$**

| Palabra | DET | SUST | VERB |
| ------- | --- | ---- | ---- |
| El      | 0.9 | 0.05 | 0.05 |
| gato    | 0.1 | 0.85 | 0.05 |
| duerme  | 0.1 | 0.1  | 0.8  |

#### 🔹 2️⃣ Llenado de la Matriz de Viterbi  
Se calcula cada celda con:  
$$
V_t(s) = \max_{s'} (V_{t-1}(s') \cdot P(s | s') \cdot P(w_t | s))
$$
Donde:d
- $V_t(s)$ es la probabilidad más alta de llegar a la palabra $w_t$ con la etiqueta $s$.
- $P(s | s')$ es la probabilidad de transición entre etiquetas.
- $P(w_t | s)$ es la probabilidad de emisión de la palabra $w_t$ con la etiqueta $s$.

##### Matriz de Viterbi Resultante:

| Estado   | El (DET) | gato (SUST) | duerme (VERB) |
| -------- | -------- | ----------- | ------------- |
| **DET**  | 0.54     | 0.0432      | 0.00346       |
| **SUST** | 0.015    | 0.1530      | 0.03672       |
| **VERB** | 0.005    | 0.0024      | 0.09792       |

#### 🔹 3️⃣ Retroceso (Backtracking)  
Para obtener la secuencia más probable, seguimos el camino con la mayor probabilidad en cada columna:
- **El** → DET
- **gato** → SUST
- **duerme** → VERB

🔹 **Resultado Final:**  
**"El/DET gato/SUST duerme/VERB"**  

**Conclusión**: La matriz de probabilidades de Viterbi (Es un algoritmo de inferencia) permite determinar de manera eficiente la mejor secuencia de etiquetas en un texto usando un modelo oculto de Markov (Es un modelo probabilistico).  

---

## 5. 🧠 Reconocimiento de Entidades Nombradas (Named Entity Recognition - NER)

El ``Reconocimiento de Entidades Nombradas (NER)`` es una tarea fundamental del ``Procesamiento del Lenguaje Natural (PLN)`` que consiste en identificar y clasificar automáticamente ``entidades mencionadas en un texto`` en categorías predefinidas como:

- 👤 **Personas** (Ej: "Gabriel García Márquez")
- 🌍 **Lugares** (Ej: "Colombia", "París")
- 🏢 **Organizaciones** (Ej: "ONU", "Google")
- 📅 **Fechas** (Ej: "12 de octubre de 2023")
- 💰 **Valores monetarios** (Ej: "$20", "100 euros")

### 🔍 ¿Para qué sirve el NER?
- 📑 Extracción de información clave en documentos.
- 🔎 Mejora de motores de búsqueda.
- 🤖 Chatbots inteligentes y asistentes virtuales.
- 📈 Análisis de medios, redes sociales y textos legales.