# ETIQUETADO MORFOSINTACTICO

## 1. ğŸ“Œ Morfosintaxis
La ``morfosintaxis`` es la rama de la lingÃ¼Ã­stica que estudia la relaciÃ³n entre la ``morfologÃ­a`` (estructura interna de las palabras) y la ``sintaxis`` (cÃ³mo se organizan las palabras en oraciones). Esto nos ayuda a determinar las partes de la oraciÃ³n llamadas ``part of speech (POS)``, y estas son las que se clasifican en ``categorÃ­as morfosintÃ¡cticas o gramaticales.``

## 2. ğŸ“Œ Â¿QuÃ© es el Etiquetado MorfosintÃ¡ctico?  
El etiquetado morfosintÃ¡ctico es el proceso de asignar una categorÃ­a gramatical a cada palabra en un texto. Se usa en PLN para tareas como anÃ¡lisis sintÃ¡ctico, traducciÃ³n automÃ¡tica y minerÃ­a de texto.  

### 2.1. ğŸ” Principales CategorÃ­as MorfosintÃ¡cticas  
1. **Sustantivos (NOUN)** â†’ Persona, lugar, cosa.  
2. **Adjetivos (ADJ)** â†’ Describen caracterÃ­sticas.  
3. **Verbos (VERB)** â†’ AcciÃ³n o estado.  
4. **Adverbios (ADV)** â†’ Modifican verbos o adjetivos.  
5. **Pronombres (PRON)** â†’ Sustituyen sustantivos.  
6. **Preposiciones (ADP)** â†’ Relacionan palabras.  
7. **Conjunciones (CONJ)** â†’ Unen frases u oraciones.  
8. **Determinantes (DET)** â†’ ArtÃ­culos y posesivos.  
9. **Interjecciones (INTJ)** â†’ Expresiones emocionales.  

### 2.2. ğŸ› ï¸ Herramientas para Etiquetado  
- **NLTK (Python)** â†’ Modelos para etiquetado en espaÃ±ol.  
- **spaCy (Python)** â†’ Modelos neuronales avanzados.  
- **Stanford NLP (Java/Python)** â†’ Soporte para espaÃ±ol.  

> Hoy en dÃ­a, la mayorÃ­a de los algoritmos de procesamiento del lenguaje natural que procesan palabras en inglÃ©s utilizan el **Penn Treebank**, que es un conjunto de 45 etiquetas.

---

## 3. ğŸ·ï¸ Etiquetado MorfosintÃ¡ctico con Modelos Ocultos de Markov (HMM)

Un ``Modelo Oculto de Markov (HMM)`` es un modelo probabilÃ­stico que representa una secuencia de estados ocultos a partir de observaciones visibles. En el contexto del ``Procesamiento del Lenguaje Natural (PLN)``, los ``estados ocultos`` son las categorÃ­as gramaticales y las ``observaciones`` son las palabras de una oraciÃ³n.  

### 3.1. ğŸ”„ Funcionamiento del HMM para Etiquetado  

1. **Definir los estados** â†’ Son las categorÃ­as gramaticales posibles (sustantivo, verbo, adjetivo...).  
2. **Probabilidades de transiciÃ³n** â†’ Probabilidad de que una categorÃ­a gramatical suceda a otra.  
3. **Probabilidades de emisiÃ³n** â†’ Probabilidad de que una palabra pertenezca a una categorÃ­a.  
4. **Algoritmo de Viterbi** â†’ Encuentra la secuencia mÃ¡s probable de etiquetas para una oraciÃ³n.  

### ğŸ“Š Ejemplo de Etiquetado con HMM

#### Frase: *El gato duerme tranquilo*  
| Palabra   | Posibles Etiquetas | Etiqueta Asignada |
| --------- | ------------------ | ----------------- |
| El        | DET, PRON          | DET               |
| gato      | SUST, VERB         | SUST              |
| duerme    | VERB, SUST         | VERB              |
| tranquilo | ADJ, ADV           | ADJ               |

ğŸ”¹ **ExplicaciÃ³n**: Se usan las probabilidades de transiciÃ³n y emisiÃ³n para determinar la secuencia mÃ¡s probable de etiquetas.  

### 3.2. ğŸ—ï¸ ConstrucciÃ³n de un Etiquetador MorfosintÃ¡ctico con un HMM Bigrama

Un ``Modelo Oculto de Markov (HMM) Bigrama`` es un modelo probabilÃ­stico en el que la categorÃ­a gramatical de una palabra depende Ãºnicamente de la categorÃ­a de la palabra anterior. Esto simplifica el modelo y lo hace eficiente para tareas de ``etiquetado morfosintÃ¡ctico``.

1. El HMM tiene tantos estados como etiquetas, ademÃ¡s de un estado inicial y final.
```
<s> en/IN un/DT vaso/NN de/IN vino/NN </s>
<s> un/DT mosquito/NN se/PRP cayÃ³/VBZ </s>
<s> y/CC vino/VBZ otro/JJ mosquito/NN </s>
```
> En este caso tiene 7 estados en el corpus y 9 en total contando la del inicio y final.

2. Luego se calcula las probabilidad de transicion de todos los estados, por ejemplo de PRP seguido de NN

$$
P(T_i | T_{i-1}) = \frac{C(NN, PRP)}{C(NN)} = \frac{1}{4} = 0.25
$$

3. Este resultado se reemplaza en una matriz.

**ConclusiÃ³n**: El etiquetado morfosintÃ¡ctico con HMM es una tÃ©cnica efectiva para analizar la estructura de los textos y es fundamental en el procesamiento del lenguaje natural.  

---

## 4. ğŸ”¢ CreaciÃ³n de la Matriz de Probabilidades de la Ruta de Viterbi

### 4.1. ğŸ“Œ Â¿QuÃ© es el Algoritmo de Viterbi?
El ``algoritmo de Viterbi`` es un mÃ©todo eficiente para encontrar la secuencia mÃ¡s probable de estados ocultos en un ``Modelo Oculto de Markov (HMM)``. En el ``etiquetado morfosintÃ¡ctico``, permite determinar la mejor secuencia de categorÃ­as gramaticales para una oraciÃ³n.

### 4.2. ğŸ—ï¸ ConstrucciÃ³n de la Matriz de Viterbi
El algoritmo utiliza una ``matriz de probabilidades`` donde las filas representan las ``categorÃ­as gramaticales`` y las columnas representan las ``palabras de la oraciÃ³n``.

**La matriz se calcula con:**
1. **Probabilidades de emisiÃ³n** â†’ Probabilidad de que una palabra tenga una etiqueta especÃ­fica.
2. **Probabilidades de transiciÃ³n** â†’ Probabilidad de que una etiqueta siga a otra.
3. **Paso de retroceso (backtracking)** â†’ Encuentra la secuencia Ã³ptima de etiquetas.

### 4.3. ğŸ“Š Ejemplo Paso a Paso
Supongamos la oraciÃ³n:  
**"El gato duerme"**  
CategorÃ­as posibles:
- **DET** â†’ Determinante
- **SUST** â†’ Sustantivo
- **VERB** â†’ Verbo

> La matrices se definen en base a un corpus inicial

#### ğŸ”¹ 1ï¸âƒ£ Definir Matrices de Probabilidad  

| CategorÃ­a | Probabilidad Inicial |
| --------- | -------------------- |
| DET       | 0.6                  |
| SUST      | 0.3                  |
| VERB      | 0.1                  |

**Probabilidad de TransiciÃ³n $P[Etiq_i | Etiq_{i-1}]$**

| De/Para | DET | SUST | VERB |
| ------- | --- | ---- | ---- |
| DET     | 0.1 | 0.8  | 0.1  |
| SUST    | 0.3 | 0.4  | 0.3  |
| VERB    | 0.5 | 0.2  | 0.3  |

**Probabilidad de EmisiÃ³n $P[Palabra | Etiqueta]$**

| Palabra | DET | SUST | VERB |
| ------- | --- | ---- | ---- |
| El      | 0.9 | 0.05 | 0.05 |
| gato    | 0.1 | 0.85 | 0.05 |
| duerme  | 0.1 | 0.1  | 0.8  |

#### ğŸ”¹ 2ï¸âƒ£ Llenado de la Matriz de Viterbi  
Se calcula cada celda con:  
$$
V_t(s) = \max_{s'} (V_{t-1}(s') \cdot P(s | s') \cdot P(w_t | s))
$$
Donde:d
- $V_t(s)$ es la probabilidad mÃ¡s alta de llegar a la palabra $w_t$ con la etiqueta $s$.
- $P(s | s')$ es la probabilidad de transiciÃ³n entre etiquetas.
- $P(w_t | s)$ es la probabilidad de emisiÃ³n de la palabra $w_t$ con la etiqueta $s$.

##### Matriz de Viterbi Resultante:

| Estado   | El (DET) | gato (SUST) | duerme (VERB) |
| -------- | -------- | ----------- | ------------- |
| **DET**  | 0.54     | 0.0432      | 0.00346       |
| **SUST** | 0.015    | 0.1530      | 0.03672       |
| **VERB** | 0.005    | 0.0024      | 0.09792       |

#### ğŸ”¹ 3ï¸âƒ£ Retroceso (Backtracking)  
Para obtener la secuencia mÃ¡s probable, seguimos el camino con la mayor probabilidad en cada columna:
- **El** â†’ DET
- **gato** â†’ SUST
- **duerme** â†’ VERB

ğŸ”¹ **Resultado Final:**  
**"El/DET gato/SUST duerme/VERB"**  

**ConclusiÃ³n**: La matriz de probabilidades de Viterbi (Es un algoritmo de inferencia) permite determinar de manera eficiente la mejor secuencia de etiquetas en un texto usando un modelo oculto de Markov (Es un modelo probabilistico).  

---

## 5. ğŸ§  Reconocimiento de Entidades Nombradas (Named Entity Recognition - NER)

El ``Reconocimiento de Entidades Nombradas (NER)`` es una tarea fundamental del ``Procesamiento del Lenguaje Natural (PLN)`` que consiste en identificar y clasificar automÃ¡ticamente ``entidades mencionadas en un texto`` en categorÃ­as predefinidas como:

- ğŸ‘¤ **Personas** (Ej: "Gabriel GarcÃ­a MÃ¡rquez")
- ğŸŒ **Lugares** (Ej: "Colombia", "ParÃ­s")
- ğŸ¢ **Organizaciones** (Ej: "ONU", "Google")
- ğŸ“… **Fechas** (Ej: "12 de octubre de 2023")
- ğŸ’° **Valores monetarios** (Ej: "$20", "100 euros")

### ğŸ” Â¿Para quÃ© sirve el NER?
- ğŸ“‘ ExtracciÃ³n de informaciÃ³n clave en documentos.
- ğŸ” Mejora de motores de bÃºsqueda.
- ğŸ¤– Chatbots inteligentes y asistentes virtuales.
- ğŸ“ˆ AnÃ¡lisis de medios, redes sociales y textos legales.