# Manejo de Datos Faltantes: Imputaci贸n

## 1. Problem谩tica de Valores Faltantes

### 1.1. El problema de trabajar con valores faltantes

- Ignorar a los valores faltantes puede introducir sesgos en tus an谩lisis y modelos.
- Multiples algoritmos disponibles fallar谩n.
- Imputaci贸n: Estimar los valores ausentes con base en los valores v谩lidos de otras variables y/o casos de muestra.

### 1.2. Procesi de an谩lisis y limpiesa de datos

[Enlace](./Ejercicios/13.imputacion_de_datos_faltantes/download-data-and-load-it.ipynb)

### 1.3. Visualizar y eliminar valores faltantes

[Enlace](./Ejercicios/13.imputacion_de_datos_faltantes/download-data-and-load-it.ipynb)

### 1.4. Implicaciones de los distintos tipos de valores faltantes

Existen 3 mecanismos de valores faltanes:

#### MCAR (Missing Complettely At Random)

La localizaci贸n de los valores faltantes en el conjunto de datos ocurren completamente al azar, estos no dependen de ning煤n otro dato.

**Eliminaci贸n de valores faltantes:**

- Reducci贸n del tama帽o de muestra.
- Inferencia limitada.
- No produce sesgos

**Imputaci贸n de valores faltantes:**

- De hacerlo bien, no produce sesgos
- La imputaci贸n es recomendada sobre la deleci贸n.

#### MAR (Missing At Random)

La localizaci贸n de los valores faltantes en el conjunto de datos dependen de otros valores observados.

**Eliminaci贸n de valores faltantes:**

- Ignorarlos produce sesgos

**Imputaci贸n de valores faltantes:**

- La mayor parte de m茅todos de imputaci贸n asumen MAR
- La imputaci贸n es necesaria

#### MNAR (Missing Not At Random)

La localizaci贸n de los valores faltantes en el conjunto de datos dependen de otros valores faltantes en s铆 mismos.

**Eliminaci贸n de valores faltantes:**

- Ignorarlos produce sesgos

**Imputaci贸n de valores faltantes:**

- La imputaci贸n no es recomendada, a no ser que sea sobre la deleci贸n.

 Mejorar experimentos o realizar an谩lisis de sensibilidad (repetir los experimentos, explorar los par谩metros, ver como cambian los an谩lisis al modificar algunos datos y establecer intervalos de confianza)

### 1.5. Ampl铆a tu conjunto de herramientas para explorar valores faltantes

[Enlace](./Ejercicios/13.imputacion_de_datos_faltantes/download-data-and-load-it.ipynb)

Aca mostramos un nuevo grafico de tres variables para poder analizar nuestros valores faltantes llamado mosaico.

### 1.6. Tratamiento de variables categ贸ricas para imputaci贸n

Para poder manejar los valores faltantes de las variables categor铆cas es recomendable codificarlos.

- **Ordinal Encoder o Codificaci贸n ordinal:** Aca se le asigna un n煤mero a cada categor铆a.
  - Ejemplo:
    |Categorias|Codificaci贸n|
    |----------|------------|
    |Perro|0|
    |Gato|1|
    |Rata|2|
    |Paloma|3|

- **One-hot encoding:** Se le asigna 1 y 0 a las categor铆as generando un codigo para cada uno.
  - Ejemplo:

    |Perro|Gato|Rata|Paloma|
    |-----|----|----|------|
    |1|0|0|0|
    |0|1|0|0|
    |0|0|1|0|
    |0|0|0|1|

[Enlace](./Ejercicios/13.imputacion_de_datos_faltantes/download-data-and-load-it.ipynb)

### 1.7. M茅todos de imputaci贸n de valores faltantes

En el manejo de valores faltantes existen dos tipos de tratamientos, pero primero entendamos lo siguiente.

> Que son las imputaciones con base en el donante?\
> Completa los valores que faltan para unidad copiando los valores observados de otra unidad, el donante.
>
> Que son las imputaciones basadas en el modelo?\
> El objetivo de la imputaci贸n basada en modelos es encontrar un modelo predictivo para cada variable objetivo en el conjunto de datos que contiene valores faltantes.

**Tratamientos de valores faltantes:**

- `Delecciones / Eliminaciones:` Aca existen diferentes m茅todos que se vio en el anterior notebook.
  - Pairwise deletion (Eliminaci贸n por parejas)
  - Listwise deletion (Eliminaci贸n por filas)
  - Eliminaci贸n completa de columnas
  - Elimina 煤nicamente los valores faltantes
  - Elimina las filas con valores faltantes
  - Elimina las columnas con valores faltantes
- `Imputaciones:` Deducen los valores faltantes.
  - General
    - Datos que no son series de tiempo
      - Imputar con una constante `(Imputaci贸n por donante)`
      - Imputar con media, mediana o moda `(Imputaci贸n por donante)`
    - Datos que son series de tiempo
      - Llenado hacia atras `(Imputaci贸n por donante)`
      - Llenado hacia adelante `(Imputaci贸n por donante)`
      - Interpolaci贸n
  - Avanzada
    - KNN `(Imputaci贸n por donante)`
    - MICE `(Imputaci贸n por modelo)`
    - NN `(Imputaci贸n por modelo)`
    - SVM `(Imputaci贸n por modelo)`
    - Otros modelos `(Imputaci贸n por modelo)`

## 2. Imputaci贸n basada en el donante

### 2.1. Imputaci贸n por media, mediana y moda

**Ventajas:**

- Rapido y facil de implementar.
- La media puede ser util en presencia de outliers.
- No afectara el estadistico en cuestion ni el tamano de la muestra.

**Desventajas:**

- Puede sesgar los resultados, dado que modifica la distribucion por debajo (curtosis). Zona donde se estan aglomerando los datos
- Pierdes correlaciones entre variables dado que no es muy preciso o muy real. Carece de variabilidad.
- No puedes usarlo en variables categoricas (a excepcion de la moda ya que puedes usar el valor mas frecuente).

### 2.2. Imputaci贸n por llenado hacia atr谩s y hacia adelante

**Ventajas:**

- Rapido y facil de implementar.
- Los datos imputados no son constantes
- Existen trucos para evitar romper las relaciones

**Desventajas:**

- Relaciones multivariables pueden ser distorsionadas

### 2.3. Imputaci贸n por interpolaci贸n

La interpolaci贸n puede tener distintas formas, y puede ser imputaci贸n por modelo o donante, ya que puede obtener valores seg煤n una l铆nea recta por modelo, o puede recibir valores de adelante o de atras.

**Ventajas:**

- Sencillo de implementar
- til para series de tiempo
- Variabilidad de opciones al alcance

**Desventajas:**

- Puede romper relaciones entre variables
- Puede introducir valores fuera de rango

### 2.4. Imputaci贸n por KNN

Pasos para imputaci贸n pork-Nearest-Neighbors

1. Encuentra otras K observaciones (donadores, vecinos) que sean m谩s similares a esa observaci贸n.
2. Reemplaza los valores faltantes con los valores agregados de los K vecinos.

Para determinar cuales son los vecinos mas similares se utilizan las metricas de distancia la cual cuantifica la distancia que hay entre dos observaciones. Entre los distintos tipos de metrica de distancia tenemos:

- Distancia Euclidiana: util para las variables numericas, se traza una linea recta entre dos puntos, porque se considera el camino mas corto en un espacio euclidiano, y esa distancia se utiliza como parametro para definir si se encuentra lejos o cerca de otro punto.

- Distancia Manhattan: es muy util para variables de tipo factor (dias de la semana, condiciones como lento, rapido), se trata de la distancia absoluta que debemos recorrer para llegar del punto a al punto b.

- Distancia de Hamming: es util para variables categoricas, cuantifica la diferencia entre los textos.

- Distancia de Gower: util para conjuntos de datos con variables mixtas, se define como la distancia de un vecino a otros vecinos para determinar la distancia global referente a la fila que contiene el valor faltante.

**Ventajas:**

- Sencillo de implementar.
- buen rendimiento con conjunto de datos pequenos.
- Excelente para datos numericos, pero tambien funciona para datos mixtos

**Desventajas:**

- Su escalabilidad puede ser comprometedora dependiendo del numero de variables y el tamano de las observaciones.
- Requiere transformaciones especiales para las variables categoricas ya que no solo estas introduciendo numeros y se necesita cuntificar estas distancias.
- Posee sensibilidad a valores atipicos

## 3. imputaci贸n basada en modelos

### 3.1. Introducci贸n a la imputaci贸n basada en modelos

**Ventajas:**

- Mejora sobre la imputacion basada en donantes sencilla
- Presenta gran variedad de opciones para imputar. Puedes elegir tecnicas de Machine Learning como puede ser un SVM o tecnicas de Deep Learning como Redes Neuronales.
- Preserva la relacion entre las variables

**Desventajas:**

- Puede subestimar la varianza. Tambien arrastra todos los defecto que presente el modelo.
- Los modelos funcionan mal si las variables observadas y faltantes son independientes.
- Es mas complicado que la imputacion basada en donantes

### 3.2. Imputaciones M煤ltiples por Ecuaciones Encadenadas (MICE)

Permite preserva las relaciones entre las variables y por tanto es preferido sobre las imputaciones simples. Consiste en tratar inicialmente con un conjunto de datos con valores faltantes, luego crea copias de dicho conjunto de datos a los que vas a imputar valores para obtener copias con valores completos, analizas los resultados y finalmente agrupas los resultados de cada conjunto de datos para dar un informe con intervalos de confianza que contribuyan a la variabilidad de los datos. Es decir primero coge una variable trata de predecir los valores faltantes en base a las demas variables y cuando este completa esta variable target sigue con el siguiente.

**Ventajas:**

- Mantiene la distribucion relativa similar antes y despues de la imputacion.
- Puede ser utilizado en cualquier tipo de analisis. Generalmente es mas utilizado para datos de tipo MCAR y MAR pero tambien para los de tipo MNAR aunque con mucha cautela.
- Multiples variables son imputadas.

**Desventajas:**

- Para funcionar bien, necesitas pensar en el modelo de imputacion y el modelo de analisis para que asi todo el estudio tenga sentido de acuerdo a los datos. No todos los algoritmos funcionan para todos los conjuntos de datos.
- Solo funciona como los metodos de imputacion seleccionados. Al iniciar el tratamiento de valores faltantes debemos definir un estimador que nos indica como vamos ajustando las variables o predecirlas en funcion del resto de variables. Por tanto necesitas pensar que estimador es el adecuado considerando sus ventajas y desventajas

### 3.3. Transformaci贸n inversa de los datos

Como hemos transformado a n煤meros nuestras variables categ贸ricas, despues de haber hecho la imputaci贸n, necesitamos regresarlo a texto.
