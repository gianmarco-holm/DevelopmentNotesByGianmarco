# CURSO DE DATA WAREHOUSING Y MODELADO OLAP

## 1. Introducci√≥n a BI y Data Warehousing

### 1.1. ¬øQu√© es BI y Data Warehousing?

#### 1.1.1. BI (Business Intelligence)

Es un conjunto de tecnolog√≠as, procesos y herramientas que permiten recopilar, analizar y presentar datos para apoyar la toma de decisiones en una organizaci√≥n. BI busca transformar datos brutos en informaci√≥n significativa, proporcionando insights para mejorar el rendimiento empresarial.

#### 1.1.2. Data Warehousing

Es el proceso de recopilaci√≥n, almacenamiento y gesti√≥n de datos de diversas fuentes en un √∫nico repositorio centralizado, conocido como almac√©n de datos. El data warehousing facilita el acceso y an√°lisis eficiente de grandes vol√∫menes de datos para respaldar las operaciones y la toma de decisiones estrat√©gicas en una organizaci√≥n.

### 1.2. Niveles de anal√≠tica y jerarqu√≠a del conocimiento

#### 1.2.1. Niveles de Anal√≠tica

* Descriptiva:

  * Describe y resume datos pasados para entender qu√© ha sucedido en el pasado.
  * Proporciona una visi√≥n retrospectiva de los eventos.

* Diagn√≥stica:

  * Analiza datos para determinar por qu√© ocurrieron ciertos eventos.
  * Identifica patrones y causas subyacentes.

* Predictiva:

  * Utiliza modelos y algoritmos para prever eventos futuros.
  * Emplea an√°lisis predictivo para anticipar tendencias y comportamientos.

* Prescriptiva:

  * Ofrece recomendaciones y acciones para influir en resultados futuros.
  * Sugiere decisiones √≥ptimas basadas en datos y modelos.

#### 1.2.2. Jerarqu√≠a del Conocimiento

* Dato:

Representaci√≥n cruda de hechos o cifras.
Carece de contexto y significado por s√≠ mismo.

* Informaci√≥n:

Datos organizados y procesados para proporcionar contexto.
Ofrece significado y relevancia.

* Conocimiento:

Informaci√≥n interpretada y entendida.
Implica la comprensi√≥n de patrones, relaciones y causas.

* Sabidur√≠a:

Aplicaci√≥n del conocimiento de manera efectiva.
Incluye la capacidad de tomar decisiones sabias y utilizar el conocimiento de manera √©tica y efectiva.

### 1.3. Conceptos de BI: Data Warehouse, Data Mart, Dimensiones y Hechos

* Data Warehouse:

  * Un repositorio centralizado que almacena grandes cantidades de datos de diversas fuentes.
  * Dise√±ado para facilitar el an√°lisis y la generaci√≥n de informes para respaldar la toma de decisiones.

* Data Mart:

  * Una versi√≥n m√°s peque√±a y espec√≠fica de un almac√©n de datos.
  * Centrado en un √°rea funcional o departamento particular de una organizaci√≥n.

* Dimensiones:

  * Caracter√≠sticas o aspectos espec√≠ficos que describen los datos almacenados.
  * Utilizadas para organizar y categorizar datos en un almac√©n de datos, proporcionando contexto.
  * Es el que nos va ayudar a responder las preguntas de negocio como el ¬øQuien?, ¬øQu√©?, ¬øComo?, ¬øCuando? y ¬øD√≥nde?

* Hechos:

  * Datos num√©ricos o m√©tricas que representan las medidas de inter√©s.
  * Asociados a dimensiones para proporcionar informaci√≥n completa y contextualizada.

Estos conceptos son fundamentales en la construcci√≥n y utilizaci√≥n de sistemas de Business Intelligence para transformar datos en informaci√≥n significativa y apoyar la toma de decisiones empresariales.

### 1.4. Base de datos OLTP vs. OLAP

| **Aspecto**         | **Base de Datos OLTP (Procesamiento de Transacciones en L√≠nea)**                                                                                                                | **Base de Datos OLAP (Procesamiento Anal√≠tico en L√≠nea)**                                                                                                |
| ------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Prop√≥sito:**      | - Dise√±ada para el procesamiento eficiente de transacciones en tiempo real. <br> - Optimizada para realizar operaciones de inserci√≥n, actualizaci√≥n y eliminaci√≥n de registros. | - Dise√±ada para el an√°lisis y la generaci√≥n de informes. <br> - Optimizada para consultas complejas y anal√≠ticas.                                        |
| **Estructura:**     | - Normalizaci√≥n de datos para minimizar la redundancia. <br> - Relacional y centrada en la consistencia de los datos.                                                           | - Desnormalizaci√≥n de datos para facilitar el an√°lisis. <br> - Utiliza estructuras multidimensionales como cubos.                                        |
| **Consulta:**       | - Maneja consultas simples y transacciones individuales. <br> - Busca mantener la integridad de los datos durante operaciones diarias.                                          | - Maneja consultas complejas que involucran grandes conjuntos de datos. <br> - Centrada en proporcionar una visi√≥n anal√≠tica y estrat√©gica de los datos. |
| **Rendimiento:**    | - Orientada a la velocidad y eficiencia de transacciones. <br> - Mantiene un alto nivel de concurrencia para admitir m√∫ltiples usuarios simult√°neos.                            | - Orientada al rendimiento de consulta y an√°lisis. <br> - Puede sacrificar la velocidad de transacci√≥n en favor de un rendimiento anal√≠tico eficiente.   |
| **Bases de datos:** | - MySQL <br> - PostgreSQL <br> - Oracle                                                                                                                                         | - Amazon Redshift <br> - Azure Synapse Analytics <br> - Google Big Query                                                                                 |

### 1.5. Metodolog√≠as de Data Warehouse

#### 1.5.1. Metodolog√≠a Inmon

![inmon](./images/metodologia-inmon.jpeg)

Como podemos observar la metodolog√≠a inmon conocida por ser la primera en el data warehousing, recopila toda la informaci√≥n de diversas fuentes en una base de datos llamada **Staging** para as√≠ no afectar a las transacciones de las bases de datos OLTP,liego con un proceso ETL se lleva la data transformada un **Data WareHouse** y luego se segmenta en los **Data Mart**.

#### 1.5.2 Metodolog√≠a Kimball

![Kimball](./images/metodologia-kimball.jpeg)

En esta metodolog√≠a no se usa Data WareHouse porque se cree que no es necesario. Adem√°s nos suguiere el siguiente flujo

![Fases Kimball](./images/kimball-fases.jpeg)

Como se observa en el flujo primero se debe determinar el proposito, los objetivos y el alcance del proyecto BI y luego sigue los requerimientos.

#### 1.5.3 Metodolog√≠a Efesto

![Metodolog√≠a Efesto](./images/hefesto-fases.jpeg)

Sabiendo estas metodolog√≠as, puedes personalizar una propia.

## 2. Modelos Dimensionales

### 2.1 Data Warehouse, Data Lake y Data Lakehouse: ¬øCu√°l utilizar?

![WareHouse vs Lake](./images/warehouse-vs-datalake.webp)}

#### Data Lakehouse

* Prop√≥sito:

Combina caracter√≠sticas de Data Warehouse y Data Lake, buscando superar las limitaciones de ambos en t√©rminos de escalabilidad y flexibilidad.

* Estructura:

Ofrece la capacidad de mantener datos estructurados y no estructurados.
Proporciona una capa de procesamiento para mejorar la calidad de los datos.

* Uso:

Puede ser una opci√≥n intermedia para aquellas organizaciones que desean aprovechar la flexibilidad de un Data Lake y la estructura de un Data Warehouse.

![WareHouse vs Lake vs lakehouse](./images/warehouse-lake-datalake.jpeg)

### 2.2 Tipos de esquemas dimensionales

![Modelo Multidimensional](./images/modelo-multidimensional.jpeg)

Como se ve en la imagen, un esquema dimensional esta compuesta por una tabla de hechos y sus dimensiones que la contextualizan, recordemos que en la tabla hecho es donde estan los datos n√∫mericos y m√©tricas que son las medidas de interes. Ahora el esquema dimensional se divide en tipos:

* Esquema Estrella (Star Schema):

  * Caracter√≠sticas:
    * Estructura centralizada con una tabla de hechos rodeada de tablas de dimensiones.
    * Las dimensiones se conectan directamente a la tabla de hechos.
  * Ventajas:
    * Dise√±o simple y f√°cil de entender.
    * Rendimiento optimizado para consultas de agregaci√≥n.

* Esquema Copo de Nieve (Snowflake Schema):

  * Caracter√≠sticas:
    * Similar al esquema estrella, pero las dimensiones se normalizan en m√∫ltiples tablas.
    * Puede haber jerarqu√≠as de dimensiones m√°s profundas.
  * Ventajas:
    * Ahorro de espacio al normalizar dimensiones.
    * Mayor consistencia en los datos dimensionales.

* Esquema Constelaci√≥n (Galaxy Schema):

  * Caracter√≠sticas:
    * M√∫ltiples tablas de hechos comparten dimensiones comunes.
    * Adecuado para situaciones donde diferentes procesos comerciales comparten algunas dimensiones.
  * Ventajas:
    * Flexibilidad para representar relaciones complejas entre hechos y dimensiones.

### 2.3 Dimensiones lentamente cambiantes

Las Dimensiones Cambiantes Lentamente (Slowly Changing Dimensions o SCD) son un concepto en la gesti√≥n de datos que se refiere a c√≥mo manejar los cambios en los atributos de las dimensiones a lo largo del tiempo. En un almac√©n de datos, las dimensiones proporcionan contexto a las medidas o m√©tricas en las tablas de hechos.

Existen diferentes tipos de dimensiones cambiantes lentamente, dependiendo de c√≥mo se gestionen los cambios en los atributos de las dimensiones:

* **Tipo 1 SCD:**

En este tipo, los atributos de la dimensi√≥n se sobrescriben con los nuevos valores cuando ocurren cambios. Esto significa que los datos hist√≥ricos no se conservan y la dimensi√≥n refleja √∫nicamente los valores m√°s recientes. Los Tipo 1 SCD son adecuados cuando la informaci√≥n hist√≥rica no es importante o cuando se puede recuperar de otras fuentes.

![Tipo 1 SCD](./images/tipo1SCD.jpeg)

* **Tipo 2 SCD:**

En este tipo, se inserta un nuevo registro en la tabla de dimensiones para representar los valores modificados de los atributos. Esto permite conservar los datos hist√≥ricos, ya que cada registro representa una versi√≥n diferente de la dimensi√≥n. Por lo general, se utilizan claves sustitutas y fechas efectivas para rastrear las diferentes versiones. Los Tipo 2 SCD se utilizan com√∫nmente cuando se requiere an√°lisis hist√≥rico y la dimensi√≥n cambia con relativa poca frecuencia.

![Tipo 2 SCD](./images/tipo2SCD.jpeg)

* **Tipo 3 SCD:**

Este tipo implica agregar columnas a la tabla de dimensiones para almacenar tanto los valores actuales como los valores anteriores de ciertos atributos. Este enfoque permite un seguimiento limitado de los cambios a lo largo del tiempo, pero puede que no capture la historia completa de la dimensi√≥n. Los Tipo 3 SCD se utilizan cuando es importante realizar un seguimiento de cambios espec√≠ficos de atributos mientras se mantiene la simplicidad en el modelo de datos.

![Tipo 3 SCD](./images/tipo3SCD.jpeg)

* **Tipo 6 SCD:**

Este tipo se le conoce como 6 por la multiplicacion de 1 X 2 X 3 ya que combina los 3 tipos en uno.

![Tipo 6 SCD](./images/tipo6SCD.jpeg)

La elecci√≥n del tipo de dimensi√≥n cambiante lentamente a utilizar depende de los requisitos espec√≠ficos del almac√©n de datos y las necesidades de an√°lisis. Es importante considerar factores como la importancia de los datos hist√≥ricos, la frecuencia de los cambios en la dimensi√≥n y las implicaciones de almacenamiento y rendimiento de cada enfoque.

Las SCD son un aspecto crucial de los almacenes de datos, ya que permiten representar datos a lo largo del tiempo, lo que permite un an√°lisis e informes hist√≥ricos precisos.

### 2.4 Tabla de hechos (fact)

* Contienen informaci√≥n cuantitativa de un proceso de negocio
* Medidas - M√©tricas
* Contiene claves for√°neas de las dimensiones

Estas son las dimenciones:

![Dimensiones](./images/tabla%20hechos%201.jpeg)

Y estas es la tabla hecho fabricada:

![Tabla hecho](./images/tabla%20hechos%202.jpeg)

### 2.5 Configuraci√≥n de herramientas para Data Warehouse y ETL

¬°Hola, te doy la bienvenida a este tutorial! Configurar√°s las bases de datos y herramientas que usaremos para el ETL y crear un data warehouse.

Usaremos PostgreSQL con la base de datos Adventureworks. Ser√° nuestra base de datos transaccional y la fuente de informaci√≥n para llevar al data warehouse.

Ejecuta las siguientes instrucciones para configurar esto:

#### Ruby

Instalaci√≥n de Ruby en Ubuntu o WSL con Ubuntu:

1. Abre la terminal de Ubuntu
2. Ejecuta el siguiente comando en la terminal para actualizar la lista de paquetes disponibles:

    `sudo apt-get update`

3. Una vez actualizada la lista de paquetes, instala Ruby ejecutando el siguiente comando en la terminal:

    `sudo apt-get install ruby-full`

4. Verifica que Ruby se haya instalado correctamente ejecutando `ruby -v` en la terminal.

#### Instalaci√≥n de Ruby en Windows

1. Descarga el instalador de Ruby desde la p√°gina oficial de Ruby para Windows: https://rubyinstaller.org/downloads/
2. Selecciona la versi√≥n de Ruby que deseas instalar.
3. Ejecuta el instalador y sigue las instrucciones del asistente de instalaci√≥n.
4. Una vez completada la instalaci√≥n, abre la l√≠nea de comandos de Windows (cmd.exe) y escribe `ruby -v` para verificar que la instalaci√≥n se haya realizado correctamente.

#### Instalaci√≥n de Ruby en macOS

1. Abre la terminal de macOS.
2. Instala Homebrew ejecutando el siguiente comando en la terminal:

    ```BASH
    /bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"
    ```

3. Una vez instalado Homebrew, ejecuta el siguiente comando en la terminal para instalar Ruby:

    `brew install ruby`

4. Verifica que Ruby se haya instalado correctamente ejecutando `ruby -v` en la terminal.

Con estos pasos ya has instalado Ruby.

#### PostgreSQL y pgAdmin o DBeaver

Estas herramientas ya deber√≠as tenerla instaladas. Si no las tienes, vuelve a revisar esta clase tutorial o sigue la documentaci√≥n de PostgreSQL. ‚¨ÖÔ∏èüí°

1. Ingresa a https://www.postgresql.org/
2. Descargar -> Descargar el instalador
3. Instala, y no selecciones StackBuilder

#### Descarga y configuraci√≥n de la base de datos AdventureWorks

1. Descarga el repositorio en https://github.com/lorint/AdventureWorks-for-Postgres

    Ejecuta el siguiente comando de Git:

    `git clone https://github.com/lorint/AdventureWorks-for-Postgres.git`

    Este repositorio contiene los archivos para crear las tablas y vistas de la base de datos.

2. Descarga [Adventure Works 2014 OLTP Script](https://github.com/Microsoft/sql-server-samples/releases/download/adventureworks/AdventureWorks-oltp-install-script.zip).

    Contiene los archivos para llenar las tablas de la base de datos.

3. Copia y pega el archivo AdventureWorks-oltp-install-script.zip en el directorio AdventureWorks-for-Postgres.

4. En tu terminal √∫bicate en el directorio AdventureWorks-for-Postgres y descomprime AdventureWorks-oltp-install-script.zip:

    ```BASH
    cd AdventureWorks-for-Postgres/
    unzip AdventureWorks-oltp-install-script.zip
    ```

5. En la terminal, ubic√°ndote en el directorio AdventureWorks-for-Postgres, ejecuta el siguiente comando para convertir los archivos csv:

    `ruby update_csvs.rb`

6. Activa la conexi√≥n con postgresql, si postgresql esta instalado en windows, puedes activar el servicio llendo a servicios y luego clic derecho, iniciar, despues creas la base de datos con psql o ingresas a la shell de postgress con el comando `psql -U postgres`:

    `sudo service postgresql start`

7. Crea la base de datos con el siguiente comando de PostgreSQL:

    `psql -c "CREATE DATABASE \"Adventureworks\";"`

    o

    `psql -c "CREATE DATABASE \"Adventureworks\";" -U postgres -h localhost`

8. Ejecuta el script que llena las tablas de la base de datos:

    `psql -d Adventureworks < install.sql`

    o

    `psql -d Adventureworks < install.sql -U postgres -h localhost`

9. Conecta tu base de datos en DBeaver o pgAdmin.

   1. Abre DBeaver o pgAdmin.

   2. Selecciona la opci√≥n para crear una nueva conexi√≥n.

   3. Selecciona PostgreSQL en la lista de bases de datos.

   4. Ingresa la informaci√≥n de conexi√≥n necesaria en la pesta√±a.

      * Host: localhost
      * Port: 5432
      * Base de datos: Adventureworks
      * Nombre de usuario: postgres
      * Password: la que tengas de tu user de postgresql.

10. Haz clic en **Test Connection** para asegurarte de que los detalles de conexi√≥n sean correctos y que puedas conectarte a la base de datos.

11. Si la prueba de conexi√≥n es exitosa, haz clic en "Finalizar" para guardar la configuraci√≥n de la conexi√≥n.

#### Configuraci√≥n de Pentaho

Esta herramienta la utilizaremos para crear las ETL de los datos transaccionales (DB Adventureworks) en Postgres a el Data Warehouse en AWS Redshift.

1. Instala pentaho https://sourceforge.net/projects/pentaho/
2. Instala Java https://www.java.com/es/download/ie_manual.jsp
3. Ejecuta Spoon

    Para acceder al programa solo hace falta abrir el archivo ‚ÄúSpoon‚Äù del tipo Batch file (con la extensi√≥n .bat).

#### Instalaci√≥n y configuraci√≥n de AWS CLI

Este servicio lo usar√°s para realizar la conexi√≥n a S3 y cargar archivos planos que luego ser√°n cargados a AWS Redshift con el comando COPY.

* Instalar AWS CLI: https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html
* Configurar AWS CLI: https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-quickstart.html

#### Configuraci√≥n de AWS Redshift

AWS Redshift ser√° utilizado como data warehouse. Ser√° el lugar donde construiremos las dimensiones, tablas de hechos y llevaremos los datos modelados y limpios que se obtuvieron del sistema transaccional.

* Crea un nuevo cl√∫ster de AWS Redshift

### 2.6. Modelado dimensional: identificaci√≥n de dimensiones, m√©tricas, preguntas de negocio y reglas de negocio

#### Pregunta de negocio

Una pregunta de negocio se refiere a una consulta espec√≠fica relacionada con las operaciones, estrategias o desaf√≠os de una empresa, buscando obtener informaci√≥n clave para la toma de decisiones.

![Identificacion de m√©tricas y dimensiones](/images/identificacion-dim.jpeg)

#### Regla de negocio

Por otro lado, una regla de negocio es una declaraci√≥n que describe una pol√≠tica, restricci√≥n o requerimiento que gu√≠a o controla las actividades dentro de una organizaci√≥n, estableciendo c√≥mo deben realizarse ciertas acciones o procesos en el contexto empresarial.

Ejemplo:

* Crear un campo con el nombre completo del cliente
* El campo de observaci√≥n del producto es demasiado largo. Recortar los primeros 100 caracteres.
* Si un vendedor tiene personas a cargo marcarlo como beneficiario del bono

### 2.7 Modelado dimensional: dise√±o de modelo

Para elaborar el dise√±o dimensional ingresamos a:

https://dbdiagram.io/home

En este caso elaboraremos el siguiente modelo dimensional, solo escribimos este codigo y el diagrama se elaborara autom√°ticamente:

```sql
Table dwh.dim_clientes {
  id_cliente int pk
  codigo_cliente varchar
  nombre varchar
  apellido varchar
  nombre_completo varchar
  numero_celular varchar
  numero_casa varchar
  numero_trabajo varchar
  ciudad_casa varchar
}
Ref: dwh.dim_clientes.id_cliente < dwh.fact_ventas.id_cliente

Table dwh.dim_productos {
  id_producto int pk
  codigo_producto varchar
  nombre varchar
  color varchar
  tamano varchar
  categoria varchar
}
Ref: dwh.dim_productos.id_producto < dwh.fact_ventas.id_producto

Table dwh.dim_territorios {
  id_territorio int pk
  codigo_territorio varchar
  nombre varchar
  continente varchar
}
Ref: dwh.dim_territorios.id_territorio < dwh.fact_ventas.id_territorio

Table dwh.dim_vendedores {
  id_vendedor int pk
  codigo_vendedor varchar
  identificacion varchar
  nombre varchar
  apellido varchar
  nombre_completo varchar
  rol varchar
  fecha_nacimiento date
  genero varchar
  ind_activo boolean
  ind_bono boolean
  fecha_inicio date
  fecha_fin date
}
Ref: dwh.dim_vendedores.id_vendedor < dwh.fact_ventas.id_vendedor

Table dwh.fact_ventas {
  id_fecha int pk
  id_territorio int pk
  id_cliente int pk
  id_vendedor int pk
  id_producto int pk
  codigo_venta_detalle varchar
  codigo_venta_encabezado varchar
  cantidad int
  valor decimal
  descuento decimal
}
```

## 3. ETL para inserci√≥n en Data WareHouse

### 3.1. Documento de mapeo para el ETL

En el contexto de data warehousing, un documento de mapeo, o "mapping document", es un registro detallado que describe c√≥mo los datos se transforman y se mueven desde las fuentes de origen hasta el almac√©n de datos. Este documento es esencial para comprender y gestionar el proceso de extracci√≥n, transformaci√≥n y carga (ETL) en un entorno de data warehousing.

[Mapeo Fisico del proyecto](https://docs.google.com/spreadsheets/d/12SBx0xv-z2e6hIqDnv-aHLeQx6kQJRaM/edit?usp=sharing&ouid=103575516771010742985&rtpof=true&sd=true)

![Mapping Document](./images/mapping-document.jpeg)

![Ejemplo](./images/ejemplo-mapping-document.jpeg)

### 3.2 Creaci√≥n del modelo f√≠sico

```SQL
  --drop table dim_clientes;
  create table dim_clientes
  (
    id_cliente integer
    ,codigo_cliente varchar(20)
    ,nombre varchar(50)
    ,apellido varchar(50)
    ,nombre_completo varchar(100)
    ,numero_telefono_celular varchar(20)
    ,numero_telefono_casa varchar(20)
    ,numero_telefono_trabajo varchar(20)
    ,ciudad_casa varchar(50)
    ,fecha_carga timestamp
    ,fecha_actualizacion timestamp
    ,primary key (id_cliente)
  )
  ;


  --drop table dim_productos;
  create table dim_productos
  (
    id_producto integer
    ,codigo_producto varchar(20)
    ,nombre varchar(50)
    ,color varchar(50)
    ,tamanio varchar(50)
    ,categoria varchar(50)
    ,fecha_carga timestamp
    ,fecha_actualizacion timestamp
    ,primary key (id_producto)
  )
  ;


  --drop table dim_territorios;
  create table dim_territorios
  (
    id_territorio integer
    ,codigo_territorio varchar(20)
    ,nombre varchar(50)
    ,continente varchar(50)
    ,fecha_carga timestamp
    ,fecha_actualizacion timestamp
    ,primary key (id_territorio)
  ) 
  ;


  --drop table dim_vendedores;
  create table dim_vendedores
  (
    id_vendedor integer
    ,codigo_vendedor varchar(20)
    ,identificaci√≥n varchar(20)
    ,nombre varchar(50)
    ,apellido varchar(50)
    ,nombre_completo varchar(50)
    ,rol varchar(50)
    ,fecha_nacimiento date
    ,genero varchar(10)
    ,ind_activo boolean
    ,fecha_inicio date
    ,fecha_fin date
    ,version integer
    ,fecha_carga timestamp
    ,primary key (id_vendedor)
  ) 
  ;


  --drop table fact_ventas;
  CREATE TABLE dwh_adventureworks.fact_ventas (
    id_venta integer NOT NULL,
    codigo_venta_detalle varchar(10) NOT NULL,
    codigo_venta_encabezado varchar(10) NOT NULL,
    id_fecha integer NULL,
    id_territorio integer NULL,
    id_cliente integer NULL,
    id_vendedor integer NULL,
    id_producto integer NULL,
    cantidad integer NULL,
    valor numeric(18,2) NULL,
    descuento numeric(18,2) NULL,
    fecha_carga timestamp NULL,
    fecha_actualizacion timestamp NULL,
    CONSTRAINT fact_ventas_pkey PRIMARY KEY (id_venta)
  )


  --drop table dim_tiempo;
  create table dim_tiempo
  (
      id_fecha int not null,
      fecha date not null, 
      dia smallint not null,
      mes smallint not null,
      anio smallint not null,
      dia_semana smallint not null,
      dia_anio smallint not null,
    PRIMARY KEY (id_fecha)
  )

  --Ejecutar luego de realizar la primera carga de datos en las dimensiones con Pentaho!!!!

  INSERT INTO dwh_adventureworks.dim_clientes
  (id_cliente, codigo_cliente, nombre, apellido, nombre_completo, numero_telefono_celular, numero_telefono_casa, numero_telefono_trabajo, ciudad_casa, fecha_carga, fecha_actualizacion)
  VALUES(-1, '-1', 'Sin Informaci√≥n', 'Sin Informaci√≥n', 'Sin Informaci√≥n', '', '', '', '', '1900/01/01 00:00:00', '1900/01/01 00:00:00');


  INSERT INTO dwh_adventureworks.dim_productos
  (id_producto, codigo_producto, nombre, color, tamanio, categoria, fecha_carga, fecha_actualizacion)
  VALUES(-1, '-1', 'Sin Informaci√≥n', '', '', '', '1900/01/01 00:00:00', '1900/01/01 00:00:00');


  INSERT INTO dwh_adventureworks.dim_territorios
  (id_territorio, codigo_territorio, nombre, continente, fecha_carga, fecha_actualizacion)
  VALUES(-1, '-1', 'Sin Informaci√≥n', '', '1900/01/01 00:00:00', '1900/01/01 00:00:00');


  INSERT INTO dwh_adventureworks.dim_vendedores
  (id_vendedor, codigo_vendedor, identificaci√≥n, nombre, apellido, nombre_completo, rol, fecha_nacimiento, genero, ind_activo, fecha_inicio, fecha_fin, version, fecha_carga)
  VALUES(-1, '-1', null, 'Sin Informaci√≥n', 'Sin Informaci√≥n', 'Sin Informaci√≥n', null, '1900/01/01 00:00:00', null, true, '1900/01/01 00:00:00', '9999/12/31 00:00:00', 1, '1900/01/01 00:00:00');
```

### 3.3 Extracci√≥n: querys en SQL

Armamos una de las tablas dimensi√≥n con un query

```SQL
  SELECT
  c.customerid as cod_cliente,
  p.firstname as nombre,
  p.lastname as apellido,
  p.firstname || ' ' || p.lastname as nombre_completo,
  case when pp.phonenumbertypeid = 1 then pp.phonenumber else null end as numero_telefono_celular,
  case when pp.phonenumbertypeid = 2 then pp.phonenumber else null end as numero_telefono_casa,
  case when pp.phonenumbertypeid = 3 then pp.phonenumber else null end as numero_telefono_trabajo,
  a.city
  FROM sales.customer c
  LEFT JOIN person.person p
  on c.personid = p.businessentityid
  LEFT JOIN person.personphone pp
  on p.businessentityid = pp.businessentityid
  LEFT JOIN person.businessentity b
  on p.businessentityid = b.businessentityid
  LEFT JOIN person.businessentityaddress b2
  on (b.businessentityid = b2.businessentityid and b2.addresstypeid = 2)
  LEFT JOIN person.address a
  on b2.addressid = a.addressid
```

### 3.4 Extracci√≥n en Pentaho
